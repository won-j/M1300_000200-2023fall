{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Duality and optimality conditions\"\n",
    "subtitle: Advanced Statistical Computing\n",
    "author: Joong-Ho Won\n",
    "date: today\n",
    "date-format: \"MMMM YYYY\"\n",
    "institute: Seoul National University\n",
    "execute:\n",
    "  echo: true  \n",
    "format:\n",
    "  revealjs:\n",
    "    toc: false\n",
    "    theme: dark\n",
    "    code-fold: false   \n",
    "    scrollable: true    \n",
    "jupyter: julia    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimality conditions\n",
    "\n",
    "We do *not* assume convexity by default here.\n",
    "\n",
    "---\n",
    "\n",
    "### Unconstrained problems\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{x}\\in \\mathbb{R}^d} f(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "\n",
    "* Zeroth-order optimality condition\n",
    "$$\n",
    "f(\\mathbf{x}^{\\star}) \\le f(\\mathbf{x}), \\quad \\forall \\mathbf{x}\\in\\mathbb{R}^d.\n",
    "$$\n",
    "    - Global optimality\n",
    "    - Not easy to check\n",
    "\n",
    "---\n",
    "\n",
    "* First-order optimality condition: assume $f$ is continuously differentiable.\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}^\\star) = 0.\n",
    "$$\n",
    "    - Also known as the stationarity condition.\n",
    "    - *Necessary* condition: $\\mathbf{x}^\\star$ may *locally* minimize/maximize $f$, or even is a saddle point.\n",
    "    - If $f$ is convex, then necessary and sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "* Second-order optimality condition: assume $f$ is twice continuously differentiable.\n",
    "$$\n",
    "\\nabla^2 f(\\mathbf{x}^\\star) \\succeq 0.\n",
    "$$\n",
    "    - *Necessary* condition: $\\mathbf{x}^\\star$ *locally* minimize $f$ (if the first-order condition also holds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Constrained problems\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    &\\text{minimize}& f_0(\\mathbf{x}) \\\\\n",
    "    &\\text{subject to}& f_i(\\mathbf{x}) \\le 0, \\quad i = 1,\\ldots,m \\\\\n",
    "    & & h_i(\\mathbf{x}) = 0, \\quad i = 1,\\ldots,p.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Assume all of these functions share some open set $U \\subset \\mathbb{R}^d$ as their domain.\n",
    "\n",
    "* Zeroth-order optimality condition\n",
    "$$\n",
    "f(\\mathbf{x}^{\\star}) \\le f(\\mathbf{x}), \\quad \\forall \\mathbf{x}\\in C = \\{\\mathbf{x}: f_i(\\mathbf{x})\\le 0,~i=1,\\dotsc,m,~h_i(\\mathbf{x})=0,~i=1,\\dotsc,p\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "* First-order optimality condition: Karush-Kuhn-Tucker (KKT) conditions\n",
    "\n",
    "> Kuhn, H. W.; Tucker, A. W., [Nonlinear Programming](https://projecteuclid.org/euclid.bsmsp/1200500249). Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability, 481--492, University of California Press, Berkeley, Calif., 1951. \n",
    "\n",
    "> Karush, W., Minima of Functions of Several Variables with Inequalities as Side Constraints. M.Sc. Dissertation. Dept. of Mathematics, Univ. of Chicago, Chicago, Illinois, 1939."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KKT conditions\n",
    "\n",
    "* Define the **Lagrangian**\n",
    "\\begin{eqnarray*}\n",
    "    \\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = f_0(\\mathbf{x}) + \\sum_{i=1}^m \\lambda_i f_i(\\mathbf{x}) + \\sum_{i=1}^p \\nu_i h_i(\\mathbf{x}).\n",
    "\\end{eqnarray*}\n",
    "The vectors $\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots, \\lambda_m)^T$ and $\\boldsymbol{\\nu} = (\\nu_1,\\ldots,\\nu_p)^T$ are called the **Lagrange multiplier vectors** or **dual variables**.\n",
    "\n",
    "* Let $\\mathbf{x}^\\star$ be a local minimizer of $f_0$ in the feasible region $C$.\n",
    "\n",
    "* Assume $f_0, f_1, \\ldots,f_m,h_1,\\ldots,h_p$ are continuously differentiable near $\\mathbf{x}^{\\star}$. \n",
    "\n",
    "---\n",
    "\n",
    "### Mangasarian-Fromovitz constraint qualification\n",
    "\n",
    "Assume \n",
    "\n",
    "1. the gradients $\\nabla f_i(\\mathbf{x}^{\\star})$, $i=1,\\dotsc,m$, be linearly independent, and\n",
    "2. there exists a vector $\\mathbf{v}$ with $\\langle \\nabla h_i(\\mathbf{x}^{\\star}), \\mathbf{v} \\rangle = 0$ for $i=1,\\dotsc,p$, and $\\langle \\nabla f_i(\\mathbf{x}^{\\star}), \\mathbf{v} \\rangle < 0$ for all inequality constraints active at $\\mathbf{x}^{\\star}$, i.e., for $i$ with $f_i(\\mathbf{x}^\\star)=0$.\\\n",
    "\n",
    "  (The vector $\\mathbf{v}$ is a tangent vector in the sense that infinitesimal motion from $\\mathbf{x}^{\\star}$ along $\\mathbf{v}$ stays within the feasible region.)\n",
    "\n",
    "---\n",
    "\n",
    "### Lagrange multiplier rule\n",
    "\n",
    "* Suppose that the objective function $f_0$ of the constrained optimization problem above has a local minimum at the feasible point $\\mathbf{x}^{\\star}$. \n",
    "\n",
    "* If $f_0$ and the constraint functions are continuously differentiable near $\\mathbf{x}^{\\star}$, and the Mangasarian-Fromovitz constraint qualification holds at $\\mathbf{x}^{\\star}$, then there exist Lagrange multipliers $\\lambda_1^{\\star}, \\dotsc, \\lambda_m^{\\star}$ and $\\nu_1^{\\star}, \\dotsc, \\nu_p^{\\star}$ such that\n",
    "$$\n",
    "\\small\n",
    "    \\nabla_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}^{\\star},\\boldsymbol{\\lambda}^{\\star}, \\boldsymbol{\\nu}^{\\star})=\n",
    "    \\nabla f_0(\\mathbf{x}^\\star) + \\sum_{i=1}^m \\lambda_i^\\star \\nabla f_i(\\mathbf{x}^\\star) + \\sum_{i=1}^p \\nu_i^\\star \\nabla h_i(\\mathbf{x}^\\star) = \\mathbf{0}.\n",
    "$$\n",
    "\n",
    "* Moreover, each of the multipliers $\\lambda_i^{\\star}$ associated with the inequality constraints is nonnegative, \n",
    "and $\\lambda_i^{\\star} = 0$ whenever $f_i(\\mathbf{x}^{\\star}) < 0$. \n",
    "\n",
    "---\n",
    "\n",
    "* We can summarize these as the **KKT conditions**\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    f_i(\\mathbf{x}^\\star) &\\le& 0, \\quad i = 1,\\ldots,m \\\\\n",
    "    h_i(\\mathbf{x}^\\star) &=& 0, \\quad i = 1,\\ldots,p \\\\\n",
    "    \\lambda_i^\\star &\\ge& 0, \\quad i = 1,\\ldots,m \\\\\n",
    "    \\lambda_i^\\star f_i(\\mathbf{x}^\\star) &=& 0, \\quad i=1,\\ldots,m \\\\\n",
    "    \\nabla f_0(\\mathbf{x}^\\star) + \\sum_{i=1}^m \\lambda_i^\\star \\nabla f_i(\\mathbf{x}^\\star) + \\sum_{i=1}^p \\nu_i^\\star \\nabla h_i(\\mathbf{x}^\\star) &=& \\mathbf{0}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* The fourth condition $\\lambda_i^\\star f_i(\\mathbf{x}^\\star) = 0$ is called the **complementary slackness**.\n",
    "\n",
    "---\n",
    "\n",
    "* Remember that the KKT conditions are collectively a *necessary* condition for *local* optimality. \n",
    "\n",
    "* If the optimization problem is *convex*, then they become a *necessary and sufficient* condition, i.e., finding a triple $(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ that satisfies the KKT conditions guarantees *global* optimiality of the triple.\n",
    "    - Many algorithms for convex optimization are conceived as, or can be interpreted as, methods for solving the KKT conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### Slater's constraint qualification condition\n",
    "\n",
    "> Assume there exists a feasible point $\\mathbf{x}_0$ such that \n",
    "$f_i(\\mathbf{x}_0) < 0$ for active inequality constraints, i.e., for all $i$ with $f_i(\\mathbf{x}^{\\star})=0$.\n",
    "\n",
    "- If $f_i$ are convex, then Slater's condition implies the Mangasarian-Fromovitz condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example: MLE of multinomial probabilities\n",
    "\n",
    "The likelihood of observation $(n_1, \\dotsc, n_r)$ from $\\text{mult}(n, (p_1, \\dotsc, p_r))$ is $\\prod_{i=1}^r p_i^{n_i}$. The MLE problem is\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "    &\\text{minimize}& -\\sum_{i=1}^r n_i\\log p_i \\\\\n",
    "    &\\text{subject to}& -p_i \\le 0, \\quad i = 1,\\dotsc,r \\\\\n",
    "    & & \\sum_{i=1}^r p_i = 1.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "---\n",
    "\n",
    "This is a convex optimization problem. The Lagrangian is \n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{p}, \\boldsymbol{\\lambda}, \\nu) \n",
    "= -\\sum_{i=1}^r n_i\\log p_i - \\sum_{i=1}^r \\lambda_i p_i + \\nu\\left(\\sum_{i=1}^r p_i - 1\\right).\n",
    "$$\n",
    "Differentiate with respect to $\\mathbf{p}$ to have\n",
    "$$\n",
    "0 = -\\frac{n_i}{p_i} - \\lambda_i + \\nu, \\quad i=1,\\dotsc, r.\n",
    "$$\n",
    "Multiply $p_i$ on both sides and sum on $i$ to have\n",
    "$$\n",
    "0 = -n - \\sum_{i=1}^r \\lambda_i p_i + \\nu \\sum_{i=1}^r p_i\n",
    "  = -n - 0 + \\nu\n",
    "$$\n",
    "due to complementary slackness. \n",
    "\n",
    "Setting all $\\lambda_i=0$ gives $p_i=n_i/n$. The triple $((n_1/n, \\dotsc, n_r/n), \\mathbf{0}, n)$ satisfies the KKT conditions and hence is a global solution (Even if some $n_i=0$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order optimality condition\n",
    "\n",
    "Suppose the Lagrangian multiplier rule is satisfied at a point $\\mathbf{x}^{\\star}$ with associated multipliers $\\boldsymbol{\\lambda}$ and $\\boldsymbol{\\nu}$. \n",
    "\n",
    "Assume the Lagrangian $\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ is twice continuously differentiable in $\\mathbf{x}$ near $\\mathbf{x}^{\\star}$. \n",
    "\n",
    "If $\\mathbf{v}^T\\nabla^2_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\\mathbf{v} > 0$ for all $\\mathbf{v}\\neq\\mathbf{0}$ satisfying $\\langle \\nabla h_i(\\mathbf{x}^{\\star}), \\mathbf{v} \\rangle = 0$ for all $i$ and $\\langle \\nabla f_i(\\mathbf{x}^{\\star}), \\mathbf{v} \\rangle \\le 0$ for all active constraints, then $\\mathbf{x}^{\\star}$ is a local minimum of $f_0$.\n",
    "\n",
    "* That is, $\\nabla^2_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}^{\\star}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ is positive definite for all tangent vectors at $\\mathbf{x}^{\\star}$.\n",
    "\n",
    "* Partial inverse to the Lagrange multiplier rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duality\n",
    "\n",
    "* The **Lagrange dual function** is the minimum value of the Langrangian over $\\mathbf{x}$:\n",
    "$$\n",
    "\\small\n",
    "    g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\inf_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\inf_{\\mathbf{x}} \\left( f_0(\\mathbf{x}) + \\sum_{i=1}^m \\lambda_i f_i(\\mathbf{x}) + \\sum_{i=1}^p \\nu_i h_i(\\mathbf{x}) \\right).\n",
    "$$\n",
    "    - No constraints are imposed on $\\mathbf{x}$ in defining $g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$.\n",
    "    - Dual function $g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ is **concave** (subject to $\\boldsymbol{\\lambda}\\ge \\mathbf{0}$) regardless of the convexity of the primal (original) problem (why?)\n",
    "\n",
    "---\n",
    "\n",
    "* Denote the optimal value of original problem by $p^\\star=\\inf_{\\mathbf{x}\\in C}f_0(\\mathbf{x})$. For any $\\boldsymbol{\\lambda} \\ge \\mathbf{0}$ and any $\\boldsymbol{\\nu}$, we have\n",
    "$$\n",
    "    g(\\lambda, \\nu) \\le p^\\star.\n",
    "$$\n",
    "Proof: For any feasible point $\\tilde{\\mathbf{x}}$, \n",
    "$$\n",
    "    \\mathcal{L}(\\tilde{\\mathbf{x}}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = f_0(\\tilde{\\mathbf{x}}) + \\sum_{i=1}^m \\lambda_i f_i(\\tilde{\\mathbf{x}}) + \\sum_{i=1}^p \\nu_i h_i(\\tilde{\\mathbf{x}}) \\le f_0(\\tilde{\\mathbf{x}})\n",
    "$$\n",
    "because the second term is non-positive (since $\\boldsymbol{\\lambda}\\ge\\mathbf{0}$) and the third term is zero. Then\n",
    "$$\n",
    "    g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) = \\inf_{\\mathbf{x}} L(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\le L(\\tilde{\\mathbf{x}}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\le f_0(\\tilde{\\mathbf{x}}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "* Since each pair $(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ with $\\boldsymbol{\\lambda} \\geq \\mathbf{0}$ gives a lower bound to the optimal value $p^\\star$. It is natural to ask for the *best possible lower bound* for $p^{\\star}$ the Lagrange dual function can provide. This leads to the **Lagrange dual problem**\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{maximize}& g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu}) \\\\\n",
    "    \\text{subject to}& \\boldsymbol{\\lambda} \\geq \\mathbf{0},\n",
    "\\end{array}    \n",
    "$$\n",
    "which is a convex problem whether or not the primal problem is convex.\n",
    "\n",
    "* Let $d^\\star=\\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}g(\\boldsymbol{\\lambda},\\boldsymbol{\\nu})$. Then we have **weak duality**\n",
    "\\begin{eqnarray*}\n",
    "    d^\\star \\le p^\\star.\n",
    "\\end{eqnarray*}\n",
    "The difference $p^\\star - d^\\star$ is called the **duality gap**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong duality\n",
    "\n",
    "- Refers to zero duality gap. \n",
    "\n",
    "- Strong duality occurs when (but not only when) the problem is convex and the Lagrange multiplier rule holds at a feasible point $\\hat{\\mathbf{x}}$. \n",
    "\n",
    "- Let the corresponding Lagrange multipliers be $(\\hat{\\boldsymbol{\\lambda}}, \\hat{\\boldsymbol{\\nu}})$. In this case, $\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})$ is convex in $\\mathbf{x}$. Since the Lagrange multiplier rule states that $\\nabla_{\\mathbf{x}}\\mathcal{L}(\\hat{\\mathbf{x}}, \\hat{\\boldsymbol{\\lambda}}, \\hat{\\boldsymbol{\\nu}}) = \\mathbf{0}$, point $\\hat{\\mathbf{x}}$ furnishes an unconstrained minimum of the Lagrangian. Hence\n",
    "$$\n",
    "p^{\\star} \\le f(\\hat{\\mathbf{x}}) = \n",
    "\\mathcal{L}(\\hat{\\mathbf{x}}, \\hat{\\boldsymbol{\\lambda}}, \\hat{\\boldsymbol{\\nu}}) \n",
    "= \\inf_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\hat{\\boldsymbol{\\lambda}}, \\hat{\\boldsymbol{\\nu}}) \n",
    "= g(\\hat{\\boldsymbol{\\lambda}}, \\hat{\\boldsymbol{\\nu}}) \n",
    "\\le d^{\\star}.\n",
    "$$\n",
    "(Why the first equality?)\n",
    "\n",
    "  That is, $d^{\\star}=p^{\\star}$.\n",
    "\n",
    "* For convex problems, conditions for the Lagrange multiplier rule holds also yields strong duality, e.g., *Slater's condition*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: QP dual\n",
    "\n",
    "Consider the dual of QP\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize}& \\frac{1}{2}\\mathbf{x}^T\\mathbf{P}\\mathbf{x} + \\mathbf{q}^T\\mathbf{x} \\\\\n",
    "    \\text{subject to}& \\mathbf{A}\\mathbf{x} = \\mathbf{b}\n",
    "\\end{array}\n",
    "$$\n",
    "($\\mathbf{P}\\succ\\mathbf{0}$).\n",
    "\n",
    "The Lagrangian is\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\nu}) = \\frac{1}{2}\\mathbf{x}^T\\mathbf{P}\\mathbf{x} + \\mathbf{q}^T\\mathbf{x} + \\boldsymbol{\\nu}^T(\\mathbf{A}\\mathbf{x}-\\mathbf{b}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "The dual function is\n",
    "$$\n",
    "\\begin{split}\n",
    "g(\\boldsymbol{\\nu}) &= \\inf_{\\mathbf{x}} \\frac{1}{2}\\mathbf{x}^T\\mathbf{P}\\mathbf{x} + \\mathbf{q}^T\\mathbf{x} + \\boldsymbol{\\nu}^T(\\mathbf{A}\\mathbf{x}-\\mathbf{b}) \\\\\n",
    "    &= \\inf_{\\mathbf{x}} \\frac{1}{2}\\mathbf{x}^T\\mathbf{P}\\mathbf{x} + (\\mathbf{A}^T\\boldsymbol{\\nu}+\\mathbf{q})^T\\mathbf{x} - \\boldsymbol{\\nu}^T\\mathbf{b} \\\\\n",
    "    &= -\\frac{1}{2}(\\mathbf{A}^T\\boldsymbol{\\nu}+\\mathbf{q})^T\\mathbf{P}^{-1}(\\mathbf{A}^T\\boldsymbol{\\nu}+\\mathbf{q}) - \\boldsymbol{\\nu}^T\\mathbf{b}.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The infimum is attained with\n",
    "$$\n",
    "\\mathbf{x} = -\\mathbf{P}^{-1}(\\mathbf{A}^T\\boldsymbol{\\nu}+\\mathbf{q}).\n",
    "$$\n",
    "\n",
    "Thus the dual problem is an *unconstraint* QP, which is easier to solve than the primal.\n",
    "\n",
    "---\n",
    "\n",
    "The maximum of $g(\\boldsymbol{\\nu})$ occurs at \n",
    "$$\n",
    "    \\mathbf{0} = -\\mathbf{b} - \\mathbf{A}\\mathbf{P}^{-1}(\\mathbf{A}^T\\boldsymbol{\\nu}+\\mathbf{q})\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    \\boldsymbol{\\nu} = -(\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{A})^{-1}(\\mathbf{b}+\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{q}).\n",
    "$$\n",
    "\n",
    "Therefore the primal optimum is attained at\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\mathbf{x}^{\\star} &= -\\mathbf{P}^{-1}(-\\mathbf{A}^T(\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{A})^{-1}(\\mathbf{b}+\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{q})+\\mathbf{q})\n",
    "    \\\\\n",
    "    &= \\mathbf{P}^{-1}\\mathbf{A}^T(\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{A})^{-1}(\\mathbf{b}+\\mathbf{A}\\mathbf{P}^{-1}\\mathbf{q}) - \\mathbf{P}^{-1}\\mathbf{q}.\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-max inequality\n",
    "\n",
    "Note\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "    &= \n",
    "    \\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\n",
    "f_0(\\mathbf{x}) + \\sum_{i=1}^m \\lambda_i f_i(\\mathbf{x}) + \\sum_{i=1}^p \\nu_i h_i(\\mathbf{x})\n",
    "    \\\\\n",
    "    &= \n",
    "    \\begin{cases}\n",
    "    f_0(\\mathbf{x}) & \\text{if } \\mathbf{x} \\in C \\\\\n",
    "    \\infty & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    = f_0(\\mathbf{x}) + \\iota_{C}(\\mathbf{x})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "C = \\{\\mathbf{x}: f_i(\\mathbf{x})\\le 0,~i=1,\\dotsc,m,~h_i(\\mathbf{x})=0,~i=1,\\dotsc,p\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Thus weak duality is \n",
    "$$\n",
    "\\begin{split}\n",
    "    d^{\\star} = \\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}} g(\\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "    &= \\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\\inf_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "    \\\\\n",
    "    &\\le\n",
    "    \\inf_{\\mathbf{x}}\\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "    \\\\\n",
    "    &= \\inf_{\\mathbf{x}\\in C} f_0(\\mathbf{x}) \n",
    "    = p^{\\star}\n",
    "\\end{split}\n",
    "$$\n",
    "or min-max inequlity.\n",
    "\n",
    "\n",
    "Strong duality amounts to prove the minimax theorem\n",
    "$$\n",
    "\\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\\inf_{\\mathbf{x}}\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu})\n",
    "=\n",
    "\\inf_{\\mathbf{x}}\\sup_{\\boldsymbol{\\lambda}\\ge\\mathbf{0}, \\boldsymbol{\\nu}}\\mathcal{L}(\\mathbf{x}, \\boldsymbol{\\lambda}, \\boldsymbol{\\nu}).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "* For some *nonconvex* problems strong duality may hold.\n",
    "* Not all *convex* problems have strong duality (usually require qualification such Slater's or Mangasarian-Fromovitz).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Boyd & Vandenberghe (Ch. 5)\n",
    "\n",
    "Lange, K., 2016. MM optimization algorithms (Vol. 147). SIAM. (Ch. 3)\n",
    "\n",
    "Lange, K., 2010. Numerical analysis for statisticians. Springer Science & Business Media. (Ch. 11)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "117.19999694824219px",
    "width": "251.60000610351562px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "399px",
    "left": "0px",
    "right": "1237.800048828125px",
    "top": "33px",
    "width": "199.8000030517578px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
