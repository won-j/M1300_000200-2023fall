{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Semidefinite Programming \"\n",
    "subtitle: Advanced Statistical Computing\n",
    "author: Joong-Ho Won\n",
    "date: today\n",
    "date-format: \"MMMM YYYY\"\n",
    "institute: Seoul National University\n",
    "execute:\n",
    "  echo: true  \n",
    "format:\n",
    "  revealjs:\n",
    "    toc: false\n",
    "    theme: dark\n",
    "    code-fold: false   \n",
    "    scrollable: true    \n",
    "jupyter: julia    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.9.3\n",
      "Commit bed2cd540a1 (2023-08-24 14:43 UTC)\n",
      "Build Info:\n",
      "  Official https://julialang.org/ release\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin22.4.0)\n",
      "  CPU: 8 × Intel(R) Core(TM) i5-8279U CPU @ 2.40GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-14.0.6 (ORCJIT, skylake)\n",
      "  Threads: 2 on 8 virtual cores\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Dropbox/class/M1399.000200/2023/M1300_000200-2023fall/lectures/19-sdp`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mStatus\u001b[22m\u001b[39m `~/Dropbox/class/M1399.000200/2023/M1300_000200-2023fall/lectures/19-sdp/Project.toml`\n",
      "  \u001b[90m[1e616198] \u001b[39mCOSMO v0.8.8\n",
      "  \u001b[90m[f65535da] \u001b[39mConvex v0.15.4\n",
      "  \u001b[90m[a93c6f00] \u001b[39mDataFrames v1.6.1\n",
      "  \u001b[90m[2e9cd046] \u001b[39mGurobi v1.2.0\n",
      "  \u001b[90m[b99e6be6] \u001b[39mHypatia v0.7.3\n",
      "  \u001b[90m[b6b21f68] \u001b[39mIpopt v1.5.1\n",
      "  \u001b[90m[b8f27783] \u001b[39mMathOptInterface v1.22.0\n",
      "  \u001b[90m[6405355b] \u001b[39mMosek v10.1.3\n",
      "  \u001b[90m[1ec41992] \u001b[39mMosekTools v0.15.1\n",
      "  \u001b[90m[91a5bcdd] \u001b[39mPlots v1.39.0\n",
      "  \u001b[90m[c946c3f1] \u001b[39mSCS v2.0.0\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(pwd())\n",
    "Pkg.instantiate()\n",
    "Pkg.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP\n",
    "\n",
    "* Consider a linear map from $\\mathbb{R}^n$ to $\\mathbb{S}^d$\n",
    "$$\n",
    "    \\mathcal{A}: \\mathbf{x} \\mapsto x_1 \\mathbf{F}_1 + \\cdots + x_n \\mathbf{F}_n.\n",
    "$$\n",
    "  A **linear matrix inequality** (LMI) refers to $\\mathcal{A}\\mathbf{x} + \\mathbf{G} \\preceq \\mathbf{0}$ or $\\mathcal{A}\\mathbf{x} + \\mathbf{G} \\succeq \\mathbf{0}$. Here, $\\mathbf{G}, \\mathbf{F}_1, \\ldots, \\mathbf{F}_n \\in \\mathbb{S}^d$.\n",
    "\n",
    "* A **semidefinite program (SDP)** has the form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathcal{A}\\mathbf{x} + \\mathbf{G} \\preceq \\mathbf{0}\\\\\n",
    "\t& & \\mathbf{A} \\mathbf{x} = \\mathbf{b},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{A} \\in \\mathbb{R}^{p \\times n}$, and $\\mathbf{b} \\in \\mathbb{R}^p$. The optimization variable is $\\mathbf{x}$.\n",
    "\n",
    "    When $\\mathbf{G}, \\mathbf{F}_1, \\ldots, \\mathbf{F}_n$ are all diagonal, SDP reduces to LP. (Why?)\n",
    "\n",
    "---\n",
    "\n",
    "* The **standard form SDP** has form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\text{tr}(\\mathbf{C} \\mathbf{X}) \\\\\n",
    "\t&\\text{subject to}& \\text{tr}(\\mathbf{A}_i \\mathbf{X}) = b_i, \\quad i = 1, \\ldots, p \\\\\n",
    "\t& & \\mathbf{X} \\succeq \\mathbf{0},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{C}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_p \\in \\mathbb{S}^d$.\n",
    "Note both the objective and the equality constraint are linear in the optimization variable $\\mathbf{X}\\in\\mathbb{S}^d$: $\\text{tr}(\\mathbf{A}\\mathbf{X})=\\langle \\mathbf{A}, \\mathbf{X} \\rangle$. (Why?)\n",
    "\n",
    "* An **inequality form SDP** has form\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& x_1 \\mathbf{A}_1 + \\cdots + x_n \\mathbf{A}_n \\preceq \\mathbf{B},\n",
    "\\end{eqnarray*}\n",
    "with variable $\\mathbf{x} \\in \\mathbb{R}^n$, and parameters $\\mathbf{B}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_n \\in \\mathbb{S}^d$, $\\mathbf{c} \\in \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semidefinite Representability\n",
    "\n",
    "* A convex set $S \\in \\mathbb{R}^n$ is said to be *semidefinite representable* (SDr) if $S$ is the projection of a set in a higher-dimensional space that can be described by a set of LMIs. Specifically,\n",
    "$$\n",
    "    \\mathbf{x}\\in S \\iff \\exists\\mathbf{u}\\in \\mathbb{R}^m: \\mathcal{A}\\begin{pmatrix}\\mathbf{x}\\\\\\mathbf{u}\\end{pmatrix} - \\mathbf{B} \\succeq \\mathbf{0}\n",
    "$$\n",
    "where $\\mathcal{A}: \\mathbb{R}^n\\times \\mathbb{R}^m$ is a linear map.\n",
    "\n",
    "* A convex function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is said to be SDr if and only if $\\text{epi}f$ is SDr.\n",
    "\n",
    "---\n",
    "\n",
    "* Any SOCr set is SDr, because\n",
    "$$\n",
    "    \\|\\mathbf{x}\\|_2 \\le t \n",
    "    \\iff\n",
    "    \\mathbf{x}^T\\mathbf{x} \\le t^2, ~ t \\ge 0\n",
    "    \\iff\n",
    "    \\begin{bmatrix} t\\mathbf{I} & \\mathbf{x} \\\\ \\mathbf{x}^T & t \\end{bmatrix} \\succeq \\mathbf{0}\n",
    "$$\n",
    "the last expression is an LMI; this is due to the Schur complement.\n",
    "    - Consequence: any SOCP is SDP.\n",
    "\n",
    "* Exercise. Write LP, QP, QCQP, and SOCP in form of SDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: nearest correlation matrix\n",
    "\n",
    "* Let $\\mathbb{C}^p$ be the convex set of $p \\times p$ correlation matrices\n",
    "$$\n",
    "\t\\mathbb{C}^p = \\{ \\mathbf{X} \\in \\mathbb{S}_+^p: x_{ii}=1, i=1,\\ldots,p\\}.\n",
    "$$\n",
    "Given $\\mathbf{A} \\in \\mathbb{S}^p$, often we need to find the closest correlation matrix to $\\mathbf{A}$\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "\t\\text{minimize}& \\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\\\\n",
    "\t\\text{subject to}& \\mathbf{X} \\in \\mathbb{C}^p.\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "This projection problem can be solved via an SDP\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "\t\\text{minimize}& t \\\\\n",
    "\t\\text{subject to}& \\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\le t \\\\\n",
    "\t & \\text{diag}(\\mathbf{X}) = \\mathbf{1} \\\\\n",
    "\t & \\mathbf{X} \\succeq \\mathbf{0}\n",
    "    \\end{array}\n",
    "$$\n",
    "in variables $\\mathbf{X} \\in \\mathbb{S}^{p}$ and $t \\in \\mathbb{R}$. \n",
    "\n",
    "* The SOC constraint $\\|\\mathbf{A} - \\mathbf{X}\\|_{\\text{F}} \\le t$ can be written as an LMI\n",
    "$$\n",
    "\t\\begin{bmatrix}\n",
    "\t\tt \\mathbf{I} & \\text{vec} (\\mathbf{A} - \\mathbf{X}) \\\\\n",
    "\t\t\\text{vec} (\\mathbf{A} - \\mathbf{X})^T & t\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}\n",
    "$$\n",
    "(why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       "  1.0        0.701548    -0.241841     0.339226   -0.491819\n",
       "  0.701548   1.0         -0.00847432   0.0815131  -0.306746\n",
       " -0.241841  -0.00847432   1.0         -0.187443   -0.335222\n",
       "  0.339226   0.0815131   -0.187443     1.0         0.0438733\n",
       " -0.491819  -0.306746    -0.335222     0.0438733   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random, LinearAlgebra, Convex\n",
    "\n",
    "Random.seed!(123) # seed\n",
    "n = 5\n",
    "A = randn(n,n)\n",
    "ludecomp = lu(A)\n",
    "P = ludecomp.L * transpose(ludecomp.L)\n",
    "Drt = Diagonal(1 ./sqrt.(diag(P)))\n",
    "C = Drt * P * Drt  # correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.130718940453724\n",
       " 0.5558237025323486\n",
       " 0.8245896298137761\n",
       " 1.4100343266239637\n",
       " 2.0788334005761886"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(C)  # positive definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       "  1.2565     0.328028    0.451559   -0.584448  -0.353937\n",
       "  0.328028   0.854993   -0.0292125  -0.21921   -0.0975128\n",
       "  0.451559  -0.0292125   0.518852   -0.664467   0.179528\n",
       " -0.584448  -0.21921    -0.664467    0.842271   0.300607\n",
       " -0.353937  -0.0975128   0.179528    0.300607   0.758908"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E  = randn(n, n) * 5e-1  \n",
    "A = C + Symmetric(E)   # perturbed correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -0.15626393041586717\n",
       "  0.47654001329122847\n",
       "  0.7290094141661763\n",
       "  0.9508610234009199\n",
       "  2.2313768576133635"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A) # indefinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20.606486 seconds (24.06 M allocations: 2.155 GiB, 3.50% gc time, 95.72% compilation time)\n"
     ]
    }
   ],
   "source": [
    "using Convex, MathOptInterface\n",
    "const MOI = MathOptInterface\n",
    "\n",
    "# Use SCS solver\n",
    "using SCS\n",
    "solver = SCS.Optimizer()  \n",
    "MOI.set(solver, MOI.RawOptimizerAttribute(\"verbose\"), 0)\n",
    "\n",
    "# Use COSMO solver\n",
    "#using COSMO\n",
    "# solver = COSMO.Optimizer()\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"max_iter\"), 10000)\n",
    "# MOI.set(solver, MOI.RawOptimizerAttribute(\"verbose\"), false)\n",
    "\n",
    "X = Semidefinite(n)\n",
    "problem = minimize(norm(vec(A - X)))  # Frobenius norm\n",
    "problem.constraints += diag(X) == ones(n)\n",
    "# Solve the problem by calling solve\n",
    "@time solve!(problem, solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       "  1.0        0.328028    0.451559   -0.584448  -0.353937\n",
       "  0.328028   1.0        -0.0292125  -0.21921   -0.0975128\n",
       "  0.451559  -0.0292125   1.0        -0.664467   0.179528\n",
       " -0.584448  -0.21921    -0.664467    1.0        0.300607\n",
       " -0.353937  -0.0975128   0.179528    0.300607   1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6335027201033284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(vec(A - X.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "This matrix is positive semidefinite up to numerical precision of the solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.1575766394919282\n",
       " 0.4004986229855604\n",
       " 0.9077188979101973\n",
       " 1.2439961037462537\n",
       " 2.2902097361445772"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(X.value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the accuracy of MOSEK (interior point solver) and SCS/COSMO (first-order solver):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.064625 seconds (7.50 M allocations: 503.126 MiB, 3.99% gc time, 98.81% compilation time: 2% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "using Convex, MathOptInterface\n",
    "const MOI = MathOptInterface\n",
    "\n",
    "## Use Mosek solver\n",
    "using Mosek, MosekTools\n",
    "solver = Mosek.Optimizer()  \n",
    "MOI.set(solver, MOI.RawOptimizerAttribute(\"LOG\"), 0) \n",
    "\n",
    "X2 = Semidefinite(n)\n",
    "problem = minimize(norm(vec(A - X2)))  # Frobenius norm\n",
    "problem.constraints += diag(X2) == ones(n)\n",
    "# Solve the problem by calling solve\n",
    "@time solve!(problem, solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6335027206974722"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(vec(A - X2.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.1575674727666177\n",
       " 0.4004830812605716\n",
       " 0.907714378195316\n",
       " 1.2440014659260892\n",
       " 2.290233601851406"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(X2.value)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* For the history of the nearest matrix problem and algorithms, check out [Nick Higham's blog](https://nhigham.com/2013/02/13/the-nearest-correlation-matrix/)\n",
    "\n",
    "* A variant of the nearest matrix problem has an intimate connection with joint estimation of Gaussian mean vector and covariance matrix:\n",
    "    - Joong-Ho Won, [Proximity Operator of the Matrix Perspective Function and its Applications](https://proceedings.neurips.cc/paper/2020/file/45f31d16b1058d586fc3be7207b58053-Paper.pdf), NeurIPS 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: eigenvalue problems\n",
    "\n",
    "* Suppose \n",
    "$$\n",
    "\t\\mathbf{A}(\\mathbf{x}) = \\mathbf{A}_0 + x_1 \\mathbf{A}_1 + \\cdots x_n \\mathbf{A}_n,\n",
    "$$\n",
    "where $\\mathbf{A}_i \\in \\mathbb{S}^m$, $i = 0, \\ldots, n$. Let $\\lambda_1(\\mathbf{x}) \\ge \\lambda_2(\\mathbf{x}) \\ge \\cdots \\ge \\lambda_m(\\mathbf{x})$ be the ordered eigenvalues of $\\mathbf{A}(\\mathbf{x})$.\n",
    "\n",
    "* Minimizing the largest eigenvalue is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "\n",
    "    - Minimizing the sum of $k$ largest eigenvalues is an SDP too. How about minimizing the sum of all eigenvalues?\n",
    "    - *Maximizing* the minimum eigenvalue is an SDP as well.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* Minimizing the *spread* of the eigenvalues $\\lambda_1(\\mathbf{x}) - \\lambda_n(\\mathbf{x})$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t_1 - t_n \\\\\n",
    "\t&\\text{subject to}& t_n \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq t_1 \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t_1, t_n \\in \\mathbb{R}$.\n",
    "\n",
    "* Minimizing the spectral radius (or spectral norm) $\\rho(\\mathbf{x}) = \\max_{i=1,\\ldots,n} |\\lambda_i(\\mathbf{x})|$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& - t \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "\n",
    "---\n",
    "\n",
    "* To minimize the condition number $\\kappa(\\mathbf{x}) = \\lambda_1(\\mathbf{x}) / \\lambda_n(\\mathbf{x})$, note $\\lambda_1(\\mathbf{x}) / \\lambda_m(\\mathbf{x}) \\le t$ if and only if there exists a $\\mu > 0$ such that $\\mu \\mathbf{I} \\preceq \\mathbf{A}(\\mathbf{x}) \\preceq \\mu t \\mathbf{I}$, or equivalently, $\\mathbf{I} \\preceq \\mu^{-1} \\mathbf{A}(\\mathbf{x}) \\preceq t \\mathbf{I}$. With change of variables $y_i = x_i / \\mu$ and $s = 1/\\mu$, we can solve the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\mathbf{I} \\preceq s \\mathbf{A}_0 + y_1 \\mathbf{A}_1 + \\cdots y_n \\mathbf{A}_n \\preceq t \\mathbf{I} \\\\\n",
    "\t& & s \\ge 0,\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{y} \\in \\mathbb{R}^n$ and $s, t \\ge 0$. In other words, we normalize the spectrum by the smallest eigenvalue and then minimize the largest eigenvalue of the normalized LMI.\n",
    "\n",
    "---\n",
    "\n",
    "* Minimizing the $\\ell_1$ norm of the eigenvalues $|\\lambda_1(\\mathbf{x})| + \\cdots + |\\lambda_m(\\mathbf{x})|$ is equivalent to the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\text{tr} (\\mathbf{A}^+) + \\text{tr}(\\mathbf{A}^-) \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A}(\\mathbf{x}) = \\mathbf{A}^+ - \\mathbf{A}^- \\\\\n",
    "\t& & \\mathbf{A}^+ \\succeq \\mathbf{0}, \\mathbf{A}^- \\succeq \\mathbf{0},\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\mathbf{A}^+, \\mathbf{A}^- \\in \\mathbb{S}_+^n$.\n",
    "\n",
    "---\n",
    "\n",
    "* Roots of determinant. The determinant of a semidefinite matrix $\\text{det} (\\mathbf{A}(\\mathbf{x})) = \\prod_{i=1}^m \\lambda_i (\\mathbf{x})$ is neither convex or concave, but **rational powers** of the determinant can be modeled using linear matrix inequalities. For a rational power $0 \\le q \\le 1/m$, the function $\\text{det} (\\mathbf{A}(\\mathbf{x}))^q$ is concave and we have\n",
    "\\begin{eqnarray*}\n",
    "\t& & t \\le \\text{det} (\\mathbf{A}(\\mathbf{x}))^q\\\\\n",
    "\t&\\Leftrightarrow& \\begin{bmatrix}\n",
    "\t\\mathbf{A}(\\mathbf{x}) & \\mathbf{L} \\\\\n",
    "\t\\mathbf{L}^T & \\text{diag}(\\mathbf{L})\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}, \\quad (\\ell_{11} \\ell_{22} \\cdots \\ell_{mm})^q \\ge t,\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{L} \\in \\mathbb{R}^{m \\times m}$ is a lower-triangular matrix. The first inequality is an LMI, and the second is SOCr.\n",
    "\n",
    "  Similarly for any rational $q>0$, we have\n",
    "\\begin{eqnarray*}\n",
    "\t& & t \\ge \\text{det} (\\mathbf{A}(\\mathbf{x}))^{-q} \\\\\n",
    "\t&\\Leftrightarrow& \\begin{bmatrix}\n",
    "\t\\mathbf{A}(\\mathbf{x}) & \\mathbf{L} \\\\\n",
    "\t\\mathbf{L}^T & \\text{diag}(\\mathbf{L})\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}, \\quad (\\ell_{11} \\ell_{22} \\cdots \\ell_{mm})^{-q} \\le t\n",
    "\\end{eqnarray*}\n",
    "for a lower triangular $\\mathbf{L}$.\n",
    "\n",
    "---\n",
    "\n",
    "* References: See Lecture 4 (p146-p151) in the book [Ben-Tal and Nemirovski (2001)](https://doi.org/10.1137/1.9780898718829) for the proof of above facts.\n",
    "\n",
    "* `lambda_max`, `lambda_min` are implemented in Convex.jl package for Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical applications of eigenvalue problems\n",
    "\n",
    "### Covariance matrix estimation\n",
    "\n",
    "Suppose $\\mathbf{x}_1, \\dotsc, \\mathbf{x}_n \\in \\mathbb{R}^p \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ and we want to estimate the covariance matrix $\\Sigma$ by maximium likelihood. If $n \\ge p$, then the sample covariance matrix $\\mathbf{S}=\\frac{1}{n}\\sum_{i=1}^n\\mathbf{x}_i\\mathbf{x}_i^T$ is the MLE. However, if $n < p$ (high-dimensional setting), the MLE is not defined (why?)\n",
    "  \n",
    "  A possiblity is to *regularize* the MLE, e.g., by restricting the condition number of $\\boldsymbol\\Sigma$ to be less than $\\kappa$. This approach leads to an SDP\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "    \\text{maximize} & \\log\\text{det}\\boldsymbol{\\Sigma}^{-1} - \\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\mathbf{S}) \\\\\n",
    "    \\text{subject to} & \\mu\\mathbf{I} \\preceq \\boldsymbol{\\Sigma}^{-1} \\preceq \\kappa\\mu\\mathbf{I}\n",
    "    \\end{array}\n",
    "$$\n",
    "where the optimization variables are $\\boldsymbol{\\Sigma}^{-1}\\in\\mathbb{S}^p$ and $\\mu\\in\\mathbb{R}_+$. See\n",
    "\n",
    "- Won, J.H., Lim, J., Kim, S.J. and Rajaratnam, B., 2013. [Condition‐number‐regularized covariance estimation](https://doi.org/10.1111/j.1467-9868.2012.01049.x). Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(3), pp.427-450. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Optimal experiment design    \n",
    "\n",
    "Consider a linear model\n",
    "$$\n",
    "y_i = \\mathbf{x}_i^T\\boldsymbol{\\beta} + \\varepsilon_i, \\quad i=1,\\dotsc,n,\n",
    "$$\n",
    "where $\\varepsilon_i$ are independent Gaussian noises with common variance $\\sigma^2$. \n",
    "It is well known that the least squares estimate $\\hat{\\boldsymbol{\\beta}}$ is unbiased and has covariance $\\sigma^2(\\sum_{i=1}^n\\mathbf{x}_i\\mathbf{x}_i ^T)^{−1}$. \n",
    "\n",
    "* In experimental design, given total number of allowable experiments, we want to choose among a list of $m$ candidate design points $\\{\\mathbf{x}_1,\\dotsc,\\mathbf{x}_m\\}$ such that the covariance matrix is *minimized* in some sense. This *exact* design problem is a combinatorial optimization problem, which is hard to solve. \n",
    "\n",
    "* Instead, weight $w_i$ is put to each design point $\\mathbf{x}_i$, and we want to find a probability vector $\\mathbf{w}\\in\\Delta_{m-1}$, and the matrix $V = (\\sum_{i=1}^m w_i\\mathbf{x}_i\\mathbf{x}_i^T)^{−1}$ is \"small\". \n",
    "\n",
    "---\n",
    "\n",
    "* Some popular criteria are:\n",
    "    - D-optimal design: minimize $\\text{det}V$.\n",
    "    - E-optimal design: minimize $\\|V\\|_2 = \\lambda_{\\max}(V)$.\n",
    "    - A-optimal design: minimize $\\frac{1}{m}\\sum_{i=1}^p\\lambda_i(V) = \\text{trace}{V}$.\n",
    "    \n",
    "* All of these problems are can be cast as SDPs; see\n",
    "    - Vandenberghe, L., Boyd, S. and Wu, S.P., 1998. [Determinant maximization with linear matrix inequality constraints](https://doi.org/10.1137/S0895479896303430). SIAM Journal on Matrix Analysis and Applications, 19(2), pp.499-533. \n",
    "    - Papp, D., 2012. [Optimal designs for rational function regression](https://doi.org/10.1080/01621459.2012.656035). Journal of the American Statistical Association, 107(497), pp.400-411. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: singular value problems\n",
    "\n",
    "* Let $\\mathbf{A}(\\mathbf{x}) = \\mathbf{A}_0 + x_1 \\mathbf{A}_1 + \\cdots x_n \\mathbf{A}_n$, where $\\mathbf{A}_i \\in \\mathbb{R}^{p \\times q}$ and $\\sigma_1(\\mathbf{x}) \\ge \\cdots \\sigma_{\\min\\{p,q\\}}(\\mathbf{x}) \\ge 0$ be the ordered singular values.\n",
    "\n",
    "* **Spectral norm** (or **operator norm** or **matrix-2 norm**) minimization.  Consider minimizing  the spectral norm $\\|\\mathbf{A}(\\mathbf{x})\\|_2 = \\sigma_1(\\mathbf{x})$. Note $\\|\\mathbf{A}\\|_2 \\le t$ if and only if $\\mathbf{A}^T \\mathbf{A} \\preceq t^2 \\mathbf{I}$ (and $t \\ge 0$) if and only if $\\begin{bmatrix} t\\mathbf{I} & \\mathbf{A} \\\\ \\mathbf{A}^T & t \\mathbf{I} \\end{bmatrix} \\succeq \\mathbf{0}$. This results in the SDP\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& t \\\\\n",
    "\t&\\text{subject to}& \\begin{bmatrix} t\\mathbf{I} & \\mathbf{A}(\\mathbf{x}) \\\\ \\mathbf{A}(\\mathbf{x})^T & t \\mathbf{I} \\end{bmatrix} \\succeq \\mathbf{0}\n",
    "\\end{eqnarray*}\n",
    "in variables $\\mathbf{x} \\in \\mathbb{R}^n$ and $t \\in \\mathbb{R}$.\n",
    "  \n",
    "---\n",
    "\n",
    "* **Nuclear norm** minimization. Minimization of the *nuclear norm* (or *trace norm*) $\\|\\mathbf{A}(\\mathbf{x})\\|_* = \\sum_i \\sigma_i(\\mathbf{x})$ can be formulated as an SDP. \n",
    "\n",
    "  Singular values of $\\mathbf{A}$ coincides with the eigenvalues of the symmetric matrix\n",
    "$$\n",
    "    \\bar{\\mathbb{A}} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\\mathbf{0} & \\mathbf{A} \\\\\n",
    "\t\\mathbf{A}^T & \\mathbf{0}\n",
    "\t\\end{bmatrix},\n",
    "$$\n",
    "which has eigenvalues $(\\sigma_1, \\ldots, \\sigma_p, - \\sigma_p, \\ldots, - \\sigma_1)$. Therefore minimizing the nuclear norm of $\\mathbf{A}$ is same as minimizing the $\\ell_1$ norm of eigenvalues of $\\bar{\\mathbf{A}}$, which we know is an SDP.\n",
    "\n",
    "* Minimizing the sum of $k$ largest singular values is an SDP as well.\n",
    "\n",
    "* `operator_norm` and `nuclear_norm` are implemented in `Convex.jl`.\n",
    "\n",
    "---\n",
    "\n",
    "### Statistical applications\n",
    "\n",
    "* Matrix completion (we've seen this before).\n",
    "\n",
    "* (Approximately) rank-regularized covariance matrix estimation.\n",
    "$$\n",
    "    \\begin{array}{ll}\n",
    "    \\text{maximize} & \\log\\text{det}\\boldsymbol{\\Omega} - \\text{tr}(\\boldsymbol{\\Omega}\\mathbf{S})  \\\\\n",
    "    \\text{subject to} & \\|\\boldsymbol{\\Omega}\\|_* \\le k\n",
    "    \\end{array}\n",
    "$$\n",
    "where $\\boldsymbol{\\Omega}=\\boldsymbol{\\Sigma}^{-1}$.\n",
    "    - This approach yields $\\mathbf{S} + \\kappa\\mathbf{I}$ as a solution. See\n",
    "    \n",
    "    > Warton, D.I., 2008. Penalized normal likelihood and ridge regularization of correlation and covariance matrices. Journal of the American Statistical Association, 103(481), pp.340-349. <https://doi.org/10.1198/016214508000000021>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: cone of nonnegative polynomials\n",
    "\n",
    "* Consider nonnegative polynomial of degree $2n$ \n",
    "$$\n",
    "f(t) = \\mathbf{x}^T \\mathbf{v}(t) = x_0 + x_1 t + \\cdots x_{2n} t^{2n} \\ge 0, \\text{ for all } t.\n",
    "$$\n",
    "The cone\n",
    "$$\n",
    "\\mathbf{K}^n = \\{\\mathbf{x} \\in \\mathbb{R}^{2n+1}: f(t) = \\mathbf{x}^T \\mathbf{v}(t) \\ge 0, \\text{ for all } t \\in \\mathbb{R}\\}\n",
    "$$\n",
    "can be characterized by LMI\n",
    "$$\n",
    "f(t) \\ge 0 \\text{ for all } t \\Leftrightarrow x_i = \\langle \\mathbf{X}, \\mathbf{H}_i \\rangle, i=0,\\ldots,2n, \\mathbf{X} \\in \\mathbb{S}_+^{n+1},\n",
    "$$\n",
    "where $\\mathbf{H}_i \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ are [Hankel matrices](https://en.wikipedia.org/wiki/Hankel_matrix) with entries $(\\mathbf{H}_i)_{kl} = 1$ if $k+l=i$ or 0 otherwise. Here $k, l \\in \\{0, 1, \\ldots, n\\}$. \n",
    "\n",
    "---\n",
    "\n",
    "* Similarly the cone of nonnegative polynomials on a finite interval\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{K}_{a,b}^n = \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: f(t) = \\mathbf{x}^T \\mathbf{v}(t) \\ge 0, \\text{ for all } t \\in [a,b]\\}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "can be characterized by LMI as well. \n",
    "    * (Even degree) Let $n = 2m$. Then\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        \\mathbf{K}_{a,b}^n &=& \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: x_i = \\langle \\mathbf{X}_1, \\mathbf{H}_i^m \\rangle + \\langle \\mathbf{X}_2, (a+b) \\mathbf{H}_{i-1}^{m-1} - ab \\mathbf{H}_i^{m-1} - \\mathbf{H}_{i-2}^{m-1} \\rangle, \\\\\n",
    "        & & \\quad i = 0,\\ldots,n, \\mathbf{X}_1 \\in \\mathbf{S}_+^m, \\mathbf{X}_2 \\in \\mathbb{S}_+^{m-1}\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "    * (Odd degree) Let $n = 2m + 1$. Then\n",
    "    $$\n",
    "    \\begin{eqnarray*}\n",
    "        \\mathbf{K}_{a,b}^n &=& \\{\\mathbf{x} \\in \\mathbb{R}^{n+1}: x_i = \\langle \\mathbf{X}_1, \\mathbf{H}_{i-1}^m - a \\mathbf{H}_i^m \\rangle + \\langle \\mathbf{X}_2, b \\mathbf{H}_{i}^{m} - \\mathbf{H}_{i-1}^{m} \\rangle, \\\\\n",
    "        & & \\quad i = 0,\\ldots,n, \\mathbf{X}_1, \\mathbf{X}_2 \\in \\mathbb{S}_+^m\\}.\n",
    "    \\end{eqnarray*}\n",
    "    $$\n",
    "\n",
    "* References: [Nesterov (2000)](https://doi.org/10.1007/978-1-4757-3216-0_17) and Lecture 4 (p157-p159) of [Ben-Tal and Nemirovski (2001)](https://doi.org/10.1137/1.9780898718829). See also [Mosek Modelling Cookbook](https://docs.mosek.com/MOSEKModelingCookbook-letter.pdf), Sect. 6.2.6.\n",
    "\n",
    "---\n",
    "\n",
    "* Example. Polynomial curve fitting. We want to fit a univariate polynomial of degree $n$\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\tf(t) = x_0 + x_1 t + x_2 t^2 + \\cdots x_n t^n\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "to a set of measurements $(t_i, y_i)$, $i=1,\\ldots,m$, such that $f(t_i) \\approx y_i$. Define the Vandermonde matrix\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\mathbf{A} = \\begin{pmatrix}\n",
    "\t1 & t_1 & t_1^2 & \\cdots & t_1^n \\\\\n",
    "\t1 & t_2 & t_2^2 & \\cdots & t_2^n \\\\\n",
    "\t\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "\t1 & t_m & t_m^2 & \\cdots & t_m^n\n",
    "\t\\end{pmatrix},\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Then we wish $\\mathbf{A} \\mathbf{x} \\approx \\mathbf{y}$. Using least squares criterion, we obtain the optimal solution $\\mathbf{x}_{\\text{LS}} = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{y}$. With various constraints, it is possible to find optimal $\\mathbf{x}$ by SDP.\n",
    "\n",
    "* Nonnegativity. Then we require $\\mathbf{x} \\in \\mathbf{K}_{a,b}^n$.\n",
    "* Monotonicity. We can ensure monotonicity of $f(t)$ by requiring that $f'(t) \\ge 0$ or $f'(t) \\le 0$. That is $(x_1,2x_2, \\ldots, nx_n) \\in \\mathbf{K}_{a,b}^{n-1}$ or $-(x_1,2x_2, \\ldots, nx_n) \\in \\mathbf{K}_{a,b}^{n-1}$.\n",
    "* Convexity or concavity. Convexity or concavity of $f(t)$ corresponds to $f''(t) \\ge 0$ or $f''(t) \\le 0$. That is $(2x_2, 2x_3, \\ldots, (n-1)nx_n) \\in \\mathbf{K}_{a,b}^{n-2}$ or $-(2x_2, 2x_3, \\ldots, (n-1)nx_n) \\in \\mathbf{K}_{a,b}^{n-2}$.\n",
    "    \n",
    "* `nonneg_poly_coeffs()` and `convex_poly_coeffs()` are implemented in cvx. Not in Convex.jl yet.\n",
    "\n",
    "---\n",
    "\n",
    "### Statistical Applications\n",
    "\n",
    "* Kim, S.-J., Lim,  J. , Won, J. H. Nonparametric Sharpe Ratio Function Estimation in Heteroscedastic Regression Models via Convex Optimization. PMLR 84:1495-1504, 2018. <http://proceedings.mlr.press/v84/kim18b.html>\n",
    "\n",
    "* Papp, D. and F. Alizadeh (2014). Shape-constrained estimation using nonnegative splines. Journal of Computational and Graphical Statistics 23(1), 211– 231. <https://doi.org/10.1080/10618600.2012.707343>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SDP relaxation of binary LP\n",
    "\n",
    "* Consider a binary linear programming problem\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b}, \\quad \\mathbf{x} \\in \\{0,1\\}^n.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "Note\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t& & x_i \\in \\{0,1\\},~\\forall i\n",
    "\t\\Leftrightarrow x_i^2 = x_i ,~\\forall i\n",
    "\t\\Leftrightarrow \\mathbf{X} = \\mathbf{x} \\mathbf{x}^T, \\text{diag}(\\mathbf{X}) = \\mathbf{x}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "By relaxing the rank 1 constraint on $\\mathbf{X}$, we obtain an SDP relaxation\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t&\\text{minimize}& \\mathbf{c}^T \\mathbf{x} \\\\\n",
    "\t&\\text{subject to}& \\mathbf{A} \\mathbf{x} = \\mathbf{b}, \\text{diag}(\\mathbf{X}) = \\mathbf{x}, \\mathbf{X} \\succeq \\mathbf{x} \\mathbf{x}^T.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Note $\\mathbf{X} \\succeq \\mathbf{x} \\mathbf{x}^T$ is equivalent to the LMI \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\t\\begin{bmatrix}\n",
    "\t1 & \\mathbf{x}^T \\\\ \\mathbf{x} &\\mathbf{X}\n",
    "\t\\end{bmatrix} \\succeq \\mathbf{0}.\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "* This SDP can be efficiently solved and provides a **lower bound** to the original problem (why?). If the optimal $\\mathbf{X}$ has rank 1, then it is a solution to the original binary LP.\n",
    "\n",
    "* We can tighten the relaxation by adding other constraints that cut away part of the feasible set, without excluding rank 1 solutions. For instance, $0 \\le x_i \\le 1$ and $0 \\le X_{ij} \\le 1$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "180px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "390.3333435058594px",
    "left": "0px",
    "right": "818px",
    "top": "140.6666717529297px",
    "width": "180px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
